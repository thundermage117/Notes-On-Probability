\documentclass{article}

\begin{document}
\section{Probability Space}
There are often various approaches to probability each with its own advantages
and disadvantages.

Experiment is a procedure that can be infinitely repeated and has a
well-defined set of possible outcomes, known as the sample space.

The observation/result of the experiment are termed as outcomes.

\subsection{Classical Approach}
        Probability of an event $E$ is defined to be: $$P(E)=\frac{Number\; of\;
         outcomes\;in\; E}{Total\; number\; of\; outcomes}$$
         Some examples are tossing a coin or rolling a die.
         Disadvantages:
         \begin{itemize}
             \item Unable to model biases. It says nothing about cases where no
             physical symmetry exists.
             \item Doesn't deal with cases where total outcomes are infinite.
         \end{itemize}

         \subsection{Frequentist Approach}
         Also known as the relative frequency approach or frequentism. It defines
         an event's probability as the limit of its relative frequency in many
         trials.

         Probability is defined to be:
         $$ P(E)=\lim_{n \to \infty} \frac{n_E}{n}$$
         where an experiment is conducted $n$ times and event $E$ occurs $n_E$
         times.
         Disadvantages:
         \begin{itemize}
             \item It isn't efficient to conduct an experiment multiple times
             just to find the probability of an event occuring.
             \item It is unable to deal with subjective belief. Eg: Suppose a
             cricket expert says there is a $50\%$ of RCB winning the IPL this
             year. It doesn't mean that the RCB has won half the titles in the
             past.
         \end{itemize}
%
         \subsection{Axiomatic Approach}

         \subsubsection{Probability Space}

The triple ($S$, $F$, $P$) is referred to as a probability space where:
\begin{itemize}
    \item $S$ : Sample space, set of all possible outcomes of the experiment.
    \item $F$ : Event Space, subset of the sample space
    \item $P$ : Probability Measure
\end{itemize}
\subsubsection{Sample Space}

$S$ can either be finite or countably infinite or uncountably infinite.
%28/05

Examples for S:
\begin{itemize}
        \item Finite Sample Space: Single coin toss $ S=\{H,T\} $ and two coin tosses $ S=\{HH,HT,TT,TH\} $.
        \item Countably Infinite Sample Space: Keep tossing a coin till you get a head $ S=\{H,TH,TTH...\}$.
        \item Uncountably Infinite Sample Space: We have a circular dart board and we are measuring the angle at which a dart hits the board. $S=[0,2\pi]$
\end{itemize}

\subsection{Event Space}
Collection of events is called an event space, there are some properties to be satisified such as:
it has to be a ``Sigma Field''.

\subsubsection{Sigma Field}
A sigma field (or sigma algebra) F is a collection of subsets of S which satisfies the following properties:
\begin{itemize}
    \item $S \in F$
    \item If $E \in F$, then $E^C \in F$
    \item If $E_1,E_2,E_3 \cdots \in F $, then $\bigcup_{i=1}^{\infty} E_i \in F$
\end{itemize}

\subsubsection{Examples for Event Space}
\begin{itemize}
    \item Smallest possible event space:
    $$ F =\{ \phi,S\} $$
    \item Next non-trivial event space:
    $$ F=\{ \phi,E ,E^c ,S\}$$
    \item If $E_1 \in F$ and $E_2 \in F$, then $E_1 \cap E_2 \in F$ (Proof in 1.4.3)
    \item For S=$\{1,2,3,4,5,6\}$ , $E_1 ={1,2}$ and $E_2={3,4}$. The smallest event space containing $E_1$ and $E_2$ is:
    $$ F=\{\phi , S, E_1 , E_{1}^{C},E_2 , E_{2}^{C},E_1 \cup E_2 , (E_{1} \cup E_{2})^{C}\}$$
\end{itemize}

\subsubsection{Proposition 1}
A$_1$, A$_2$, A$_3$,....A$_n$ $\in$ $F$, then $\bigcap\limits_{i=1}^n$A$_i$ $\in$ $F$.

\textbf{Proof: }If A$_1$, A$_2$, A$_3$,....A$_n$ $\in$ $F$, then A$_1^c$, A$_2^c$, A$_3^c$,....A$_n^c$ $\in$ $F$ and $\bigcup\limits_{i=1}^n$A$_i^c$ $\in$ $F$. Then by property 2, $(\bigcup\limits_{i=1}^n$A$_i^c)^c$ = $\bigcap\limits_{i=1}^n$A$_i$ $\in$ $F$.

\subsubsection{Proposition 2}
A, B $\in$ $F$, then A$\setminus$B = A - B $\in$ $F$.\medskip

\textbf{Proof: }If B $\in$ $F$, then B$^c$ $\in$ $F$ by  property 2. So, A $\cap$ B$^c$ = A$\setminus$B $\in$ $F$ (As seen in proposition 1).

\subsection{Probability Measure}
The probability measure P is a function returning an event's probability. A probability is a real number between zero and one.
$$ P:F\rightarrow [0,1]$$

P has to satisfy the following 3 axioms:
\begin{itemize}
    \item $P(E) \geq 0$
    \item $P(S)=1$
    \item If $E_1,E_2 \cdots \in F$ such that $E_i \cap E_j =\phi$ then:
    $$ P(\bigcup_{i=1}^{\infty} E_{i})= \sum_{i=1}^{\infty} P(E_i)$$
\end{itemize}

For two disjoint sets:
$P(E_1 \cup E_2)= P(E_1)+P(E_2)+ \sum P(\phi)$

We will later see that $P(\phi)$ is indeed 0.

\subsection{Derived Properties of Probability}
\begin{enumerate}
    \item $$P(E^C)=1-P(E)$$
        Proof:$$E \cup E^C = S$$
            $$P(E)+P(E^C)=1$$

    \item For any two events $E_1$ and $E_2$,
    $$ P(E_1 \cup E_2)= P(E_1)+P(E_2)-P(E_1 \cap E_2)$$
    Proof:
    $$ P(E_1 \cup E_2)=P(E_1)+P(E_2 \cap E_1^C)$$
    Now, $$ E_2=(E_2 \cap E_1)\cup (E_2 \cap E_1^C)$$
    $$ \Rightarrow P(E_2)=P(E_1 \cap E_2)+ P(E_1^C \cap E_2)$$
    Also,
    $$ P(E_1 \cup E_2)= P(E_1)+P(E_1^C \cap E_2)$$
    Substitute the required value in the final equation.
\end{enumerate}

Question:
$ S=\{1,2,3,4,5,6\}$. 1 and 5 are equally likely and probability of getting a 6 is one-third.

Find minimum and maximum probability that we get an even number.

Answer:$$ Minimum \;Prob. = \frac{1}{3} \;\; when \;\; P_2 = P_4 = 0$$
$$ Maximum \;Prob. = 1,  \;\; P_2+P_4 =\frac{2}{3}  $$

%31/05 notes

\section{Conditional Property}

Given that an event $A$ has occured.

$ (S,F,P) \rightarrow$ Original probability space

If additional info has been given that A has occured, probability space need to be suitably modified.

eg: $S=\{1,2,3,4,5,6\},\;E_1=\{1,2\},\;E_2=\{3,4\}$
$$ F=\{\phi , S, E_1 , E_{1}^{C},E_2 , E_{2}^{C},E_1 \cup E_2 , (E_{1} \cup E_{2})^{C}\}$$
and event $A=E_1^c = \{3,4,5,6\}$ has occured.

Then, $ F_A= \{ \phi, A, \{3,4\},\{5,6\} \} $.(shown later)


\subsection{Modified probability space}

Then,
\begin{itemize}
    \item $S_A = A$. (Modified Sample Space)

    \item $F_A = \{ (E \cap A) | E \in F \} \rightarrow E \cap A \in F$. (Modified Event Space)

    (Also if some event $C \cap A = \phi $, then $C$ won't occur)



    To prove: $F_A$ also satisfies event space axioms.(see sec. 1.4.1)

    \begin{enumerate}
        \item $A \in F_A \rightarrow S\cap A = A$(S was original sample space)
        $\Rightarrow A \in F_A$

        \item $D \in F_A \Rightarrow D = E \cap A, D^c \in F_A$

        As,

        $D^c = A \setminus D= E^c \cap A \in F \quad(Because\;  E^c \in F)$

        \item $$D_1,D_2,\cdots \in F_A$$
        $$ (E_1 \cap A), \cdots \in F_A$$
        $$ E_1, E_2 \cdots \in F$$
        $$  \Rightarrow \bigcup_{i=1}^{\infty} E_i \in F_A$$
        $$ \Rightarrow  (\bigcup_{i=1}^{\infty} E_i) \cap A \in F_A$$
        $$\Rightarrow  (\bigcup_{i=1}^{\infty} E_i \cap A) \in F_A \Rightarrow  \bigcup_{i=1}^{\infty} D_i \in F_A$$

    \end{enumerate}
    Hence, $F_A$ is an event space.

    \item Modified probability measure

    $$ P(E/A)= \frac{P(E \cap A)}{P(A)}$$

    This definition is called conditional probability measure for any $E \in F$.

    eg:
    $ F_A = \{ \phi, \{3,4,5,6\}, \{ 3,4\}, \{5,6\} \}$, then $P(\{ 3,4\} / \{3,4,5,6\})=1/2$

    Now, we need to prove $ P(E/A)$  satisfies the 3 axioms of probability measure.(see sec. 1.5)
    \begin{itemize}
        \item $ P(E/A) \geq 0$ (as ratio of two nos. which are positive)
        \item $ P(S/A)=1$
        \item $ B_1, B_2 \cdots$ are all mutually disjoint.
        $$ P(\bigcap_{i=1}^{\infty} B_i /A) = \frac{ P(\bigcup_{i=1}^{\infty} B_i \cap A)}{P(A)}$$
        $$ =\frac{\sum_{i=1}^{\infty} P(B_i \cap A)}{P(A)} = \sum_{i=1}^{\infty} P(B_i /A)$$
        $$  \Rightarrow P(\bigcap_{i=1}^{\infty} B_i /A) = \sum_{i=1}^{\infty} P(B_i /A)$$
    \end{itemize}
\end{itemize}





\section{Total probability theorem}

Events $A_1,\cdots, A_n \in F$ which are all mutually exclusive/disjoint and exhaustive. Then,
$$ A_i \cap A_j = \phi \quad \forall\; i,j$$
$$ \bigcup_{i=1}^{n} A_i =S$$

$$ P(B)= \sum_{i=1}^{n} P(B/A_i)P(A_i)$$

Expresses $ P(B)$  in terms of conditional probability $ P(B/A_i) $ \& prior probability $P(A_i)$

Proof:
$$ B= \bigcup_{i=1}^{n} (B \cap A_i)$$

$A_i$'s are disjoint, so $(B \cap A_i)$ are also disjoint.

$$ P(B)= \sum_{i=1}^{n} P(B \cap A_i)=\sum_{i=1}^{n} P(B / A_i)P(A_i)$$

\subsection{Question}
Two factories manufacture zoggles. 20\% of $F_1$ are defective. 5\% of $F_2$ are defective.

In any week, $F_1$ produces twice the number of zoggles as $F_2$. What is the probability that a zoggle chosen randomly in a week is defective?

$$ P(D)=P(F_1)P(D/F_1)+P(F_2)P(D/F_2)$$
$$ = \frac{2}{3} \times \frac{1}{5} + \frac{1}{3} \times \frac{1}{20} = \frac{3}{20}$$

%2/6

\section{Bayes Theorem}
$A_1, \cdots, A_n $ are events which are mutually exclusive and exhaustive.
$B$ be an arbitrary event. Then,
$$ P(A_i/B)= \frac{P(B/A_i)P(A_i)}{\sum_{i=1}^n P(B/A_i)P(A_i)}$$

\begin{itemize}
    \item $P(A_i)$- Prior probability
    \item $P(B/A_i)$- Likelihood
    \item $P(A_i/B)$- Posterior probability
\end{itemize}

Bayes theorem expresses posterior probabilities $P(A_i/B)$ in terms of prior probabilities  $P(A_i)$ and likehoods  $P(B/A_i)$.

In some experiments $P(A_i)$ are all same, $P(A_i)=1/n$. Then posterior probabilities are proportional to likelyhoods:
$$ P(A_i/B)= \frac{P(B/A_i)}{\sum_{i=1}^n P(B/A_i)}$$
$$ \Rightarrow P(A_i/B) \propto P(B/A_i)$$

Proof:
$$ P(A_i/B) = \frac{P(A_i \cap B)}{P(B)}$$
$$ = \frac{P(B/A_i)P(A_i)}{\sum_{i=1}^{n} P(B/A_i)P(A_i)}$$

\subsubsection{Example}
In answering a question in a multiple choice test, a student knows the answer with probability $p$ and guesses the answer otherwise. If he/she guesses from $m$ choices, the probability of
%incomplete

Ans: $A_1=$ knowing the answer. $A_2=$ Guessing the answer.

$B= $Answer is correct. $A_1 \subseteq B$

$P(A_1)= p, P(A_2)= 1-p, P(B/A_1)= 1, P(B/A_2)= \frac{1}{m}$

$$ P(A_1/B)= \frac{P(B/A_1)P(A_1)}{P(B/A_1)P(A_1)+ P(B/A_2)P(A_2)}$$
$$ = \frac{p}{p + (1-p)\frac{1}{m}}$$

\section{Indepedent Events}
$A \;\&\; B$ are said to be independent. If
$$ P(A \cap B) = P(A)P(B)$$

In terms of conditional probability,
$$ P(B/A)= \frac{P(A \cap B)}{P(A)} = \frac{P(A)P(B)}{P(B)} = P(B)$$

$$ \Rightarrow P(B/A) = P(B)$$

Probability of event $B$ remains same with or without conditioning on $A$. Hence, $B$ is said to be independent of $A$.
Knowledge of occurence of event $A$ does not give any information about $B$.

If $A\; \&\; B $ are independent, then
$ A \;\&\; B^c$ are also independent.

Proof:
$$ P(B^c / A)= \frac{P(B^c \cap A)}{P(A)}= \frac{P(A)- P(A \cap B)}{P(A)}= \frac{P(A)- P(A)P(B)}{P(A)} = P(B^c)$$

\subsection{Important Results}
\begin{enumerate}
    \item $P(A/B)$ may be greater than, less than or equal to $P(A)$.
    \item Independent events and mututally exclusive events are different.

    Indepedence: $P(A \cap B)= P(A)P(B)$

    Mututally exclusive: $A \cap B = \phi$

    Eg 1: (Indepedent but not mutually exclusive). Coin toss followed by throwing dice experiment.

    $$ S=\{ (H,1), (H,2), \cdots, (H,6), (T,1), \cdots, (T,6)\}$$
    $F=$ Power set of S (Always an event space)

    $A=\{(H,1), \cdots, (H,6) \}$ %incomplete

    $P(A)= 1/2 \; \& \; P(B)= 1/2$
    $$ P(A \cap B)= P(\{ (H,2), (H,4), (H,6)\})= \frac{3}{12}= \frac{1}{4}$$
    $$ P(A\cap B)= P(A)P(B)$$

    Eg 2: (Not independent but mutually exclusive)
    If the events are mutually exclusive $\Rightarrow$ they are not independent.

    Single Coin Toss: $A= \{H \}$, $B= \{T \}$ $\rightarrow$ Mututally exclusive.

    $P(A \cap B)= 0$, $P(A)P(B)= \frac{1}{4}$

    Eg 3: (Not independent and not mutually exclusive) %incomplete hw
\end{enumerate}

\subsection{Conditionally indepedent events}

$A \; \& \: B$ are said to be conditionally indepedent given $C$ if
$$ P((A \cap B)/ C)= P(A/C)P(B/C)$$
In terms of conditional probabilities,
$$ P(B/C)= \frac{P((A\cap B )/ C)}{P(A/C)}= \frac{\frac{P(A \cap B \cap C)}{P(C)}}{\frac{P(A \cap C)}{P(C)}} = \frac{P(A\cap B\cap C)}{P(A \cap C)}$$ %incomplete

Indepedent Events: $P(A \cap B)= P(A)P(B)$ \& $P(B/A)= P(B)$.

Conditionally indepedent events: $P(A\cap B / C)= P(A/C)P(B/C)$ \& $P(B/(A\cap ))$

\subsubsection{Example}
Two fair coins are tossed, $S=\{HH, HT, TH, TT \}$, $A=\{HH, HT \} $, $B=\{HH, TH\} $, $C=\{HH \}$ \& $D=\{HT, TH\}$

%incomplete

\begin{enumerate}
    \item
    \item
    \item Are $A$ \& $B$ conditionally indepedent given $D$?

    $$P(A/D)= \frac{P(A\cap D)}{P(D)}= \frac{\frac{1}{4}}{\frac{1}{2}}=\frac{1}{2} $$
    %incomplete
\end{enumerate}

If $A$ and $B$ are independent, it doesn't imply $A$ and $B$ will be conditionally indepedent given $C$ and vice-versa.






\end{document}
